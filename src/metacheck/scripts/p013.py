from typing import Dict
import requests
import re
from urllib.parse import urlparse
from metacheck.utils.pitfall_utils import extract_metadata_source_filename


def is_valid_url_format(url: str) -> bool:
    """
    Check if URL has a valid format.
    """
    if url is None:
        raise ValueError("URL cannot be None")

    if not url or not isinstance(url, str):
        return False

    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False


def check_url_status(url: str, timeout: int = 10) -> Dict:
    """
    Check if URL returns a valid response (not 404 or other error).
    """
    result = {
        "is_accessible": False,
        "status_code": None,
        "error": None
    }

    if not is_valid_url_format(url):
        result["error"] = "Invalid URL format"
        return result

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)
        result["status_code"] = response.status_code

        if (200 <= response.status_code < 300) or response.status_code == 301:
            result["is_accessible"] = True

    except requests.exceptions.RequestException as e:
        result["error"] = str(e)
    except Exception as e:
        result["error"] = f"Unexpected error: {str(e)}"

    return result


def extract_urls_from_requirements(requirement_text: str) -> list:
    """
    Extract URLs from software requirement text.
    """
    if not requirement_text:
        return []

    # URL patterns
    url_patterns = [
        r'https?://[^\s<>"\']+',  # HTTP/HTTPS URLs
        r'www\.[^\s<>"\']+',  # URLs starting with www
    ]

    urls = []
    for pattern in url_patterns:
        matches = re.findall(pattern, requirement_text, re.IGNORECASE)
        urls.extend(matches)

    cleaned_urls = []
    for url in urls:
        url = re.sub(r'[,;.!?)]$', '', url)  # Remove trailing punctuation
        if url:
            cleaned_urls.append(url)

    return cleaned_urls


def detect_invalid_software_requirement_pitfall(somef_data: Dict, file_name: str) -> Dict:
    """
    Detect when metadata files have software requirements pointing to invalid pages.
    """
    result = {
        "has_pitfall": False,
        "file_name": file_name,
        "invalid_urls": [],
        "source": None,
        "metadata_source_file": None,
        "requirement_text": None
    }

    if "software_requirements" not in somef_data:
        return result

    req_entries = somef_data["software_requirements"]
    if not isinstance(req_entries, list):
        return result

    metadata_sources = ["codemeta.json", "description", "composer.json", "package.json", "pom.xml", "pyproject.toml",
                        "requirements.txt", "setup.py"]

    for entry in req_entries:
        source = entry.get("source", "")
        technique = entry.get("technique", "")

        is_metadata_source = (
                technique == "code_parser" and
                any(src in source.lower() for src in metadata_sources)
        )

        if is_metadata_source:
            if "result" in entry and "value" in entry["result"]:
                req_value = entry["result"]["value"]

                # Handle different value formats
                requirement_text = ""
                if isinstance(req_value, str):
                    requirement_text = req_value
                elif isinstance(req_value, list):
                    requirement_text = " ".join(str(item) for item in req_value)
                elif isinstance(req_value, dict):
                    for key in ["name", "value", "description", "text"]:
                        if key in req_value:
                            requirement_text += str(req_value[key]) + " "

                if requirement_text:
                    urls = extract_urls_from_requirements(requirement_text)

                    if urls:
                        invalid_urls = []

                        for url in urls:
                            url_status = check_url_status(url)

                            if not url_status["is_accessible"]:
                                invalid_urls.append({
                                    "url": url,
                                    "status_code": url_status["status_code"],
                                    "error": url_status["error"]
                                })

                        if invalid_urls:
                            result["has_pitfall"] = True
                            result["invalid_urls"] = invalid_urls
                            result["source"] = source
                            result["metadata_source_file"] = extract_metadata_source_filename(source)
                            result["requirement_text"] = requirement_text
                            break

    return result